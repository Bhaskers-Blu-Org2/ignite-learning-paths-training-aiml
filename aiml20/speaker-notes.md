# <a name="aiml20-speaker-notes"></a>AIML20：演讲者备注

PPT 演示备注可在此处获取：[presentations.md](https://github.com/microsoft/ignite-learning-paths-training-aiml/blob/master/aiml20/presentations.md)

相关的演示脚本可在以下位置找到： https://github.com/microsoft/ignite-learning-paths-training-aiml/tree/master/aiml20 。 从 `DEMO%20Setup.md` 开始。

## <a name="slide-notes"></a>幻灯片备注

幻灯片仅按标题标识。

### <a name="slide-microsft-ignite-the-tour"></a>幻灯片：Microsft Ignite the Tour

演示前的开题幻灯片

### <a name="slide-using-pre-built-ai-to-solve-business-problems"></a>幻灯片：使用预构建的 AI 来解决业务问题

大家好，我是 \<姓名和所属单位\>。 今天我将探讨在无需大量计算能力，甚至无需 AI 专业知识的情况下，可如何向应用程序添加人工智能功能。
为此，我们将使用云端提供的预构建的 AI 服务，甚至对它们自定义来满足我们的特定需求。

（将此幻灯片上的内容替换成你的详细联系信息。）

### <a name="slide-resources"></a>幻灯片：资源

在今天的课程中，我将带来很多信息和资源的链接，但如果你在我讲课的过程中没有记下它们没关系，因为你可以在这里显示的研讨会资源中心找到所有这些详细信息。 还会有多个演示，全部都可以亲自动手操作，因为这一 Github 存储库中提供了完整的源代码，包括一键将所有内容部署到 Azure 的按钮。 我会在演示结束时再次展示这张幻灯片，大家到时候可以拍照。

### <a name="slide-adding-humanlike-capabilities-to-apps"></a>幻灯片：向应用添加与人相似的功能

我们将使用预构建的 AI 服务向应用程序添加类似于人的功能，但这是什么意思呢？ 下面是一些例子。

### <a name="slide-enhance-apps-with-humanlike-capabilities"></a>幻灯片：借助与人类似的功能强化应用

[单击] 你可以向应用添加语音功能，比如说添加一个聊天界面。
 
[单击] 你可以向应用添加视觉功能，让它能理解图像的内容。

[单击] 你可以为你的应用赋予直觉，让它能理解到用户最可能想要执行的操作并自动优化用户界面。

[单击] 你可以向应用添加理解功能，使其能够与使用任何语言的用户沟通。

[单击] 或者，可以将扫描数据流中是否存在异常这一人工过程自动化，并使应用进行相应的操作。

这些只是一些示例。

### <a name="slide-overview-of-azure-cognitive-services"></a>幻灯片：Azure 认知服务的概述

我们谈到了很多关于 AI 技术可如何提供帮助的方面。 但要是否需要大量数据和大量技术专业知识才能实现它？

不。 你可依赖 Microsoft Research 的专业能力，该部门利用了我们丰富的数据存储库和 AI 专家，在 Azure 中创建了现成的 AI 服务，通过简单的 REST API 调用即可使用。 

这就是 Azure 认知服务。

### <a name="slide-azure-cognitive-services"></a>幻灯片：Azure 认知服务

Azure 认知服务包含二十多个 API；最宽泛地来说，它们提供类似于以下三类人类能力的功能：

* 视觉：理解照片、绘画、文本、手写资料和视频中的内容

* 语音：用于理解和识别语音并生成类人自然口语的工具。

* 语言:理解书面文档和文本的内容，并翻译成不同的人类语言。

* 决策：这是 Azure 认知服务的一个全新类别，主要关于像人一样对数据、内容和应用程序用户界面作出选择。

* 搜索：回答用自然语言表述的关于大型非结构化存储库的内容的问题。 

### <a name="slide-azure-cognitive-services-with-service-names"></a>幻灯片：Azure 认知服务（含服务名称）

我们在学习路径 AIML10 中的上一讲中介绍了“搜索”类别。 在这一讲中，我们会介绍其他一些可用服务，并用它们来改进零售商的网站：[单击]

计算机视觉：我们用它来分析产品照片的内容。

自定义视觉：我们用它来识别零售商销售的具体产品。

个性化体验创建服务：它通过观察客户的偏好自动采用调整网站的布局，即使对于匿名用户，也先展示最佳产品类别。

有关设置和使用认知服务的原则对所有 API 来说都相同，所以你今天在这里学到的知识适用于你想要使用的任何 AI 服务。

### <a name="slide-computer-vision"></a>幻灯片：计算机视觉

首先，我们来看一下为计算机视觉预构建的 AI，看看可以如何向应用程序赋予视觉能力，然后对其进行自定义以满足特定需求。

### <a name="slide-shop-by-photo"></a>幻灯片：按图购物

这是 Tailwind Traders 的网站，它是一家硬件零售商。 Traders 网站有很多常见的电商功能，比如可以浏览产品目录、在线订购产品和查找零售店内产品。
但它还有一些由 AI 支持的功能，下面我们就来看一下。

你可能已经猜到了，Tailwind Traders 是一家虚构的公司，这意味着我可以提供所有源代码，让你亲自部署这一应用。 可以通过这张幻灯片底部的链接找到它。

### <a name="slide-demo-shop-by-photo"></a>幻灯片：演示：按图购物

演示：“定义问题：按图购物功能失效”

让我们转到 Tailwind Traders 实时网站。 [点击声]

AI 支持的其中一项功能称为“按图购物”。 这项功能的构想是，客户可以上传他们想要购买的产品的照片，让应用显示该产品是否有库存。 接下来，让我试一试。 我们将上传这张我们感兴趣的电钻的图片，Tailwind Traders 应用会分析该图片，识别出它是一个电钻，然后向我显示 Tailwind Traders 在出售的电钻以及我可在商店中的哪个位置找到它。

但是，让我们用其他图片来试一下。 我要返回主页，再次使用“按图购物”功能，但这次选择这张钳子的图片。 遗憾的是，应用分析图片后，将其识别为锤子。 这一结果明显不尽人意，因此我们现在来看看是否能找到问题原因，想办法使用计算机视觉纠正问题。 

### <a name="slide-how-computer-vision-works"></a>幻灯片：计算机视觉的工作原理

但在这样做之前，有必要深入了解一点理论知识，理解计算机视觉的工作原理。 但不用担心，涉及到的数学知识非常少，而稍微了解一点计算机视觉的工作原理有助于我们了解问题原因及解决方法。

### <a name="slide-tasks-xkcd-comic"></a>幻灯片：任务（XKCD 漫画）

（暂停 10 秒）

在不久前，能够向计算机提供一张照片，而计算机能够反过来告诉我们有关图片内容的有用信息，这简直就是科幻小说里的内容。 这篇 XKCD 漫画发布于 2014 年 9 月。 而在 5 年后的现在，由于大数据、GPU 计算和卷积神经网络的出现，计算机很容易就能判断出一张照片的主题是不是鸟。 让我们来看看如何实现的吧。

### <a name="slide-how-neural-networks-work-brandon-rohrer"></a>幻灯片：神经网络的工作原理 (Brandon Rohrer)

这份说明是在获得 Brandon Rohrer 的授权后改编的，他经营着一个很棒的博客和视频教程系列，上面深度阐释了 AI 和机器学习的诸多方面。 请查看 Brandon 的博客了解更多详细信息；我在这里只有时间简略讲一下。

### <a name="slide-computer-vision--convolutional-neural-network"></a>幻灯片：计算机视觉/卷积神经网络

你可能听说过 AI 是由名为“深度学习”的技术提供支持的。
“深度学习”中的“深度”并不是“深奥”的意思，它只是表示图像在被分析时会通过有多个层次的卷积神经。 就是这样。

现在屏幕上显示了一个很简单的神经网络。 它只有五个层次，而实际的视觉系统有几十个层次，甚至可能是上百个。 这个网络旨在将一张图像作为输入，然后将该图像分类为下面 4 个对象中的一个且只能是其中一个：狗、自行车、苹果和网球。 就是这样：它没法检测出其他任何类型的对象，除非它已经过训练可识别它们。

### <a name="slide-trained-convolutional-nn"></a>幻灯片：经过训练的卷积神经网络

当神经网络受过训练后，它会在网络中逐层传递输入图像，在每一层将图像转换成不同的、更小的图像。 每一层都重新组合在上一层中生成的图像，而图像越来越小，直到最后只有一个值介于 0 到 1 之间的像素。 该值表示卷积神经认为该图像代表给定对象的置信度：数字越大，置信度越高。

在本例中，我们输入了一张自行车的图像，右侧的“自行车”节点的值最大。 因此，看起来这个卷积神经已经过良好训练，可检测出自行车（至少能检测出这一个）。 但是，如何“训练”神经网络呢？图像又是如何一路转换的呢？

在网络的每一个节点（每一个圆圈），都会向图像应用一个筛选器。
这与 Snapshot 滤镜或 Instagram 滤镜的概念很相似，但它不是执行将图像调为深褐色或向所有人脸添加兔耳等有用的操作，而是进行一些不一样的操作，一些在训练过程中决定的操作。 让我们来详细了解具体是什么。

### <a name="slide-filters-1"></a>幻灯片：筛选器 (1)

我们来想一个简单的图像。 这是一张十字架的图像。 其大小是 9x9 像素，其中白色为“+1”，黑色为“-1”。 我们将要对该图像应用一个筛选器，例如在神经网络中的每个节点处发生什么情况。

### <a name="slide-filters-2"></a>幻灯片：筛选器 (2)

为了转换此图像，我们将应用 3x3 的权重网格。 计算机视觉系统中经常用到这样的小网格：有时是 3x3 的，有时是 5x5 的，其权重在训练过程中确定。 此网格仅使用权重 -1 和 1，但一般权重是这一范围内的随机数。 [单击]

为了向图像应用权重，我们以图像中的特定像素为中心覆盖该权重网格。 [单击] 然后，将每个权重乘以每个像素值，并取平均值。 该平均值成为输出图像中的相应像素，对齐到权重网格的中心像素。

你可能已经注意到，不能将输入图像的边缘用作中心像素，因此输出图像比输入图像小一圈（两行两列）。 由于这个原因（以及其他类型的转换），图像会随着层层深入而变得越来越小，直到最终缩小到一个像素。

### <a name="slide-filters-3"></a>幻灯片：筛选器 (3)

让我们将权重网格向右下方移动两个像素。 现在，当我们将权重乘以源像素并取平均值时，我们得到一个不同的输出像素 (0.55)。 神经网络会扫描源图像各行各列并乘以权重，以便在输出图像中创建像素。

顺便一提，这个将筛选器扫过整张图像上的过程，虽然是一种简单的数学运算，却有着一个复杂的名称：卷积。 因此这种系统称为卷积神经网络。

### <a name="slide-training-an-image-classifier"></a>幻灯片：训练图像分类器

现在我们知道：神经网络中的每个节点（圆圈）都由其输入图像转换而来，转换取决于权重网格。 训练神经网络的诀窍在于如何选择这些权重，以便最终得出正确的数字。

[单击] 我们将使用训练数据来做到这一点：大量的狗、自行车、苹果和网球图像。 我们知道每张图像代表什么（因为已经有人看过它们，并进行了标记或“注释”），因此我们要做的就是选取权重，使正确的节点始终（或者至少尽可能地）获得最大值。

但在现实视觉网络中，可能有数百万个权重需要选择，有数百万个已标记的图像要进行计算。 我们将如何确定权重？

### <a name="slide-learning-backpropagation"></a>幻灯片：学习：反向传播

大多数有关机器学习的书籍从这里开始就会深入介绍数学，开始谈论“反向传播”、“学习速率”和“成本函数”等名词。 但除非你是 AI 研究人员，否则可以忽略这些，其原因有二。

首先，现在有许多出色的工具，可以利用大数据存储和 GPU 处理器等强大的计算资源为你完成所有数学运算。 在这方面，你可能听说过 Tensorflow 或 Pytorch 等工具。此学习路径的后续讲座中将详细介绍它们。 

其次，即使你理解了它们，要充分利用这些工具，你还需要大量训练数据、功能强大的计算资源，以及一群能够熟练使用它们的 AI 工程师。 相反，你可以通过 API 直接使用已经使用大量数据、计算资源和专业知识来训练神经网络的项目或公司的资源。

### <a name="slide-pre-trained-convolutional-nn"></a>幻灯片：预训练的卷积神经网络

如果只需要检测模型训练用于检测的对象类型，直接使用具有预定权重的模型即可。 只需提供你的图像，然后使用预训练模型生成的分类即可。 

有些模型不仅可以分类，还可以检测对象在图像中的位置，或者以其他方式分析图像。

### <a name="slide-demo-cognitive-services-computer-vision"></a>幻灯片：演示：认知服务计算机视觉

现在，让我们试用一个预训练的 AI 模型：认知服务计算机视觉。 此服务将分析你提供的图像，并为其检测到的对象提供标记（或分类）。 这些只是与以前的卷积神经网络右侧的最高置信度分数相关联的标签，但你现在使用的是 Microsoft 提供的强大神经网络，它能够识别数千个对象。

aka.ms/try-computervision 上有一个简单的基于 Web 的 UI 可供试用，我们现在就来试用一下。 稍后，我还将介绍如何以编程方式访问该 API。

### <a name="video-computer-vision-via-web"></a>视频：通过 Web 实现计算机视觉

[点击声] 这是认知服务计算机视觉页面。 如果在该页面上向下滚动一点，就会发现一个很不错的基于 Web 的窗体，该窗体可用于从 Web 或以本地文件形式上传图像进行分析。 接下来，让我们尝试上传这张戴着安全帽的男子的图片。 只需几秒钟，我们就能通过计算机视觉服务获得对该图片的分析结果。 左侧显示在该图像中检测到的对象，右侧显示包含详细分析的 JSON 输出。 其中包括在图像中检测到的对象的名称和位置，与图像关联的标记或标签的列表，对图像的纯语言描述（在本例中为“戴头盔的男人”）以及许多其他有用的信息。

### <a name="slide-cognitive-services-computer-vision"></a>幻灯片：认知服务计算机视觉

我们可以在输出的“对象”部分中看到，在该图像中检测到两个对象：某种头饰和一个人。

我们对“标记”部分更感兴趣，这部分提供了对整个图像的分类以及置信度分数。 在本例中，“man”之后置信度最高的分类是“headdress”，这并不是“按图购物”应用所需的：我们需要的是“hard hat”。
遗憾的是，此 API 未经训练，检测不到安全帽，只能检测到头盔，而这只是本例中置信度排名第六的分类。 稍后我们将了解如何解决此问题。

但是，如果要将视觉功能集成到应用中，而不是使用 Web 窗体，则需要以编程方式访问计算机视觉 API。
让我们来看看如何实现的吧。

### <a name="video-computer-vision-via-cli"></a>视频：通过 CLI 实现计算机视觉

可以使用任何可连接到 HTTP 终结点的语言来连接到认知服务 API，但我这里 [点击声] 是一个 bash 脚本，它使用 Azure CLI 创建资源，并使用“curl”连接到计算机视觉 API。 可以在本地 Shell 中安装 Azure CLI，但在这里，我使用 Visual Studio Code 中的“Azure 帐户”扩展来启动 Cloud Shell，这意味着无需安装任何程序。 等该 Shell 准备就绪后，就可以直接从此 bash 脚本执行命令。 

第一个命令创建一个资源组，我用它来保存对 API 进行身份验证所需的密钥。

下一步是创建密钥。 我正在创建一个综合认知服务密钥，它可以用于包括计算机视觉在内的许多服务。

然后，我们可以直接在终端显示该密钥。 [等待] 可以使用这两个密钥中的任何一个来与 API 对接，因此，我将这里的第一个密钥保存在环境变量中。

使用该密钥，我们可以连接到计算机视觉服务提供的终结点 URL，因此我们也将该 URL 保存在环境变量中。

然后可以选择要分析的图像。 在这里，我们提供一张图像的 URL，就是我们刚才看到的戴安全帽的男人图像。

现在，我们可以通过使用 curl 传入 JSON 输入，将密钥和图像 URL 传递到终结点中。 在短短几毫秒内，我们就获得了 JSON 形式的图像分析结果。 可以看到刚才在 Web 界面中看到的相同输出。

当然，我们可以使用任何喜欢的图像做到这一点。 让我们用其他图像再试一次，这次是一张电钻图片。 同样，我们可以使用 curl 将其传递给 API。 [等待] 有趣的是，与此图像关联的排名第一的标记是“相机”。很遗憾，它无法帮助我们搜索到真正的工具 - 我们要的是“电钻”。

### <a name="slide-adapting-computer-vision-models-with-your-own-data"></a>幻灯片：用你自己的数据调整计算机视觉模型

现在，你可以看出为什么计算机视觉 API 可能不是 Tailwind Traders 上的“按图购物”功能的最佳选择。 在某些情况下，其使用的视觉模型没有经过训练，无法识别 Tailwind Traders 销售的特定产品。 在其他情况下，它经过训练，但可检测到太多  不同类型的对象，并发现了错误的对象。 如你所见，给定一张钻头的图片，它返回给我们的标记是“camera”，而这是 Tailwind Traders 未在销售的产品。

幸运的是，我们可以解决这个问题。 让我们暂时回到理论知识。

假使我告诉你，有一种方法可以从经过预训练并能够识别成千上万张图片的模型入手，并可对其进行调整，使其仅识别你感兴趣的对象 -- 即使这些对象不属于原始模型训练数据的一部分，那将会怎么样！ 我知道这听起来很奇怪，但让我们来看看这可以如何实现。

### <a name="slide-transfer-learning"></a>幻灯片：迁移学习

我们现在手头有之前获得的经过训练的卷积神经网络，但不同之处在于：包含对象分类的最后一层已被剥离。 我们只剩下来自倒数第二层的图像。 我们可以忽略它们是图像（比如说 3x3 的图像）这一事实，而只是将它们视为数据。 现在，当我们将图像馈送到左侧时，我们不会获得置信度分数，而会获得一个数组或“特征”的集合，其中每个数组都有 9 个数据点。 在此示例网络中，它们被标记为 F1、F2...直至 F8。
置于左侧的每个图像都会在右侧生成一个不同的特征集合。

我们根本不知道这些特征是什么  ，但我们知道的是，它们很有用，因为它们可用于对神经网络最初  接受训练时所针对的全部图像类型进行分类。 谁知道呢：一个特征可能代表“greenness”，并且对树和网球的分类很有用。
另一个特征可能会对图像中的圆形区域数量进行计数，这在对自行车和交通灯分类时很有用。 关键在于：这些特征不是事先定义的：它们是通过从训练数据中学习  获得的，并且对于一般的  图像分类是有用的。 

诀窍在于：我们可以使用这些特征对原始网络甚至未经过识别训练的对象进行分类。

### <a name="slide-transfer-learning-training-1---with-the-hammer"></a>幻灯片：迁移学习训练（1 - 使用锤子图像）

假设我们想要一个用于识别锤子和安全帽的新模型。 我们可以在左侧传递锤子的图像，并在右侧收集特征。 在本例中，我们获得 8 个数据矢量（每个特征一个），以及 1 个对象类型的二进制指示符。 我们可以针对一个锤子的多张不同图片重复此操作，并收集每次获得的数据矢量和二进制指示符。

### <a name="slide-transfer-learning-training-2---with-the-white-hard-hat"></a>幻灯片：迁移学习训练（2 - 使用安全帽图像）

现在，让我们对安全帽的图片进行相同的处理。 同样，在每个示例中，我们针对每个图像收集 8 个数据矢量和 1 个二进制指示符。

将这些放在一起，你会得到什么？ 你会获得一个数据矢量的集合，且每个集合都有一个关联的二进制结果。 如果你已学习任何数据科学，则可猜到接下来会发生什么：我们可以生成一个简单的预测模型，例如逻辑回归或单层神经网络，以根据特征预测新对象的分类。

### <a name="slide-transfer-learning-trained-model"></a>幻灯片：传输学习已训练模型

事实证明，这样做的效果通常出奇的好。 你甚至不需要大量数据 -- 只要想要预测的类别差异很大，几十个图像通常就可以解决问题。 而且，你不需要大量计算能力即可通过数量相对较少的数据预测一百个左右的二进制结果。

当然，这只是一个小示例：你可能希望识别两个以上的对象，且基础神经网络肯定会在其倒数第二层生成远超过 8 个的特征。 但原理不变：你可以使用适度的新数据和计算能力来做到这一点，并且效果通常会很好。

### <a name="slide-microsoft-cognitive-services-custom-vision"></a>幻灯片：Microsoft 认知服务自定义视觉

当然，你不必亲自训练迁移学习模型。 你可以使用认知服务计算机视觉的高级视觉模型作为基础，并向名为“自定义视觉”的服务提供自己的图像和分类。

就像使用计算机视觉一样，你可以使用 API 以编程方式训练迁移学习模型，但自定义视觉还提供了一个方便的 Web UI 用于训练模型。 现在，让我们使用它来针对 Tailwind Traders 的“按图购物”功能训练模型。

### <a name="slide-demo-customized-object-recognition"></a>幻灯片：演示：自定义的对象识别

演示说明： https://github.com/microsoft/ignite-learning-paths-training-aiml/blob/master/aiml20/DEMO%20Custom%20Vision.md

### <a name="video-customvisionai"></a>视频：customvision.ai

[点击声] 现在，我已进入自定义视觉基于 Web 的界面。 它为我们提供了优秀的 UI，让我们可以在其中提供用于迁移学习分析的新图像。 可以在此项目中看到我已经上传了多张图片。 我上传了螺丝刀、钳子、电钻和锤子的图片，我将使用这些图片训练自定义模型。 我们还想检测 Tailwind Traders 销售的另一款产品：安全帽。 让我们单击“添加图像”，浏览到硬盘上收集了一些安全帽图片的文件夹，并全部选中这些图片，然后将其添加到服务中，同时提供“安全帽”标签用于训练。

上传这些文件需要花费一些时间，但在进行此操作时，请注意，此项目中的图像并不多：约 180 个，即五个类别中的每个类别各有数十个。 有时甚至更少。 尽管如此，由于我的五个对象类型截然不同，该模型的效果应该非常不错。

因此，让我们继续并单击“训练”按钮以开始迁移学习。 我们选择“快速训练”。 现在，它通过复杂的视觉模型运行所有这些图像，并使用迁移学习针对我们的五个类别创建预测模型。 只需几秒钟，我们的模型就做得有模有样了！
概率阈值设置了一个限制，低于该限制时我们完全无法预测任何分类。 如果我们仅接受置信度为 50% 或更高的分类，则其中 90.9% 的预测是正确的：这称为“精准率”。 整体而言，该模型正确地对 88.2% 的图像进行了分类：这称为“召回率”。 在你的应用中，请根据自己对做出错误召回以及完全不做出召回的容忍度选择阈值。 对于 Tailwind Traders，我们可将阈值设置得较低，因为向客户建议错误的产品并不是一件大事。 如果这是一个癌症检测应用，则你可能会做出其他选择。

现在，让我们使用模型之前未见过的新图像对模型进行试用。 我们通过单击“快速测试”按钮来实现此目的。 我们上传来自“test images”文件夹的新文件。 让我们用安全帽试试我们的模型。 你会看到，预测结果确实是“安全帽”，概率为 99.9%，因此，我们几乎可使用所选择的任何阈值完成此任务。

让我们试试其他图像：电钻。 我们的模型将图像识别为电钻的概率为 94.5%。 最后，让我们试试钳子的图片，其识别结果的置信度为 99.9%。

因此，即使我们只使用了不到 200 个图像对模型进行训练，我们的模型也能很好地完成任务。
那是因为我们将可能的标签约束为仅限于在 Tailwind Traders 销售的那些产品。

现在，我们对模型感到满意，可将其导出并合并到我们的应用中。 如果单击“导出”按钮，可将模型作为容器导出以用于 iOS 或 Android，或在我们的示例中，模型以通用 ONNX 格式导出。 现在，我们已将模型下载到硬盘上。

### <a name="slide-portable-deep-learning-models"></a>幻灯片：可移植深度学习模型

我们以 ONNX 格式导出了自定义模型。

ONNX 又称为 Open Neural Network Exchange，是 Microsoft 和 Facebook 推出的一种开放标准，旨在促进 AI 模型的自由交换和部署，并得到了众多应用程序和技术供应商的支持。

现在，我们已对自定义视觉模型进行训练，让我们将其集成到 Tailwind Traders 应用中。 为此，我们将使用 ONNX 运行时，这是一个开源推理引擎，可提供用于根据模型以 ONNX 格式生成预测的功能。

### <a name="slide-onnximagesearchtermpredictorcs"></a>幻灯片：OnnxImageSearchTermPredictor.cs

现在，我们已创建自定义模型，接下来可使用其 API 在应用中对其进行调用。 我们现在通过生成的 ONNX 文件创建一个新的“推理会话”，然后根据上传的图像以字符串形式生成分类标签。
接下来，我们只需将其传入 Tailwind Traders 网站的现有搜索功能中，即可显示结果。

```csharp 
var session = new InferenceSession(filePath);

...

var output = session.Run(new[] { input });
var prediction = output
    .First(i => i.Name == "classLabel")
    .AsEnumerable<string>()
    .First();
```

### <a name="slide-demo-onnx"></a>幻灯片：演示：ONNX

演示：ONNX 部署

### <a name="video-kudu"></a>视频：Kudu

[点击声] 我们刚才从自定义视觉中导出的模型实际上是一个 ZIP 文件，其中包含实际的 ONNX 文件 model.onnx，这是我们刚才创建的神经网络的文本表示形式，也是一个清单文件。 

现有的 Tailwind Traders 网站已在使用计算机视觉模型，该模型以名为 products.onnx 的 ONNX 文件的形式表示。 问题在于，该模型无法正确识别我们在 Tailwind Traders 销售的许多产品。 我们将使用刚才从自定义视觉中导出的 model.onnx 文件，将其重命名为products.onnx，并在我们的 Web 应用中替换它，以便“按图购物”功能可识别我们对其进行训练的全部五种产品。

在 Azure 门户中，可以看到运行 Tailwind Traders 网站的应用服务资源。 现在可以在此应用服务中执行的操作就是转到“开发工具”部分，然后选择“高级工具”功能。 随后会启动 Kudu 界面。 界面启动后，可以使用调试控制台浏览网站文件系统。 让我们浏览到 products.onnx 文件所在的位置：site -> w-root -> Standalone -> OnnxModels。 现在，可将其替换为我们使用自定义视觉创建的 product.onnx 文件的新版本。

现在，返回应用服务，我们可以继续并重启 Web 服务器，这将使其在“按图购物”功能中使用新的 ONNX 模型。

### <a name="video-netron"></a>视频：Netron

[点击声] 在等待网站重启时，让我们深入了解刚才安装的 ONNX 模型。 Lutz Roeder 开发了一个不错的小型 Web 应用，名为 Netron，它使我们能够检查 ONNX 文件中的神经网络。 让我们继续，打开 product.onnx 文件。 现在可以看到模型代表的神经网络的实际层。 让我们放大一点，看看顶部的输入。 该输入是一个图像。 它是一个 3 层的 RGB 图像，大小为 224x224 像素。 实际上，在将用户提供的图像提供给 ONNX 运行时之前，我必须裁剪并缩小该图像。 计算机视觉系统的视觉效果很差是一个不太光彩的秘密 -- 它们在图像分辨率相当低的环境下工作 -- 但仍然有出色的表现。

现在，让我们缩小并滚动浏览网络。 可以看到由自定义视觉创建的神经网络中的所有层，每一层都可以转换输入图像、应用筛选器并重新组合输出图像，就像本讲座前面部分所述的那样。 但是，最终到达输出层时，会看到输出是一个包含五个值的列表 -- 即我们对其进行训练的五种产品：锤子、安全帽等 -- 以及标记为“损失”的这个值，这是模型针对每个类别预测的置信度。 在你的应用中，可选择自己的阈值用于设置所需的置信度。

不管怎样，Tailwind Traders 网站现在已重启，让我们回到主页，看看新视觉模型表现如何。 让我们继续并上传图片，然后再次使用测试图像之一进行尝试，特别是之前表现不佳的钳子图像。 可以看到，该网站确实没有将其视为锤子，而是搜索“钳子”，并向我展示了其提供的所有产品。

### <a name="slide-optimizing-app-ui-with-cognitive-services-personalizer"></a>幻灯片：使用认知服务个性化体验创建服务优化应用 UI

我们还有一点时间来介绍关于预建 AI 的另一个快速示例，它来自认知服务的“决策类别”。 通过个性化体验创建服务，我们可以实时自定义应用接口，在用户最希望执行的操作与我们希望用户  执行的操作之间取得平衡。

### <a name="slide-recommended-screenshot"></a>幻灯片：建议（屏幕截图）

我们可以通过 Tailwind Traders 网站的“推荐”部分了解它的工作方式。 它显示了商店中提供的一系列部门：一个大的主要图像，再加上一些较小的图像。

个性化体验创建服务将根据称为“强化学习”的 AI 技术为我们选择这些部分的显示方式。

### <a name="slide-personalizer-in-action"></a>幻灯片：个性化体验创建服务实践

多年来，Microsoft 一直致力于个性化体验创建服务的开发。 它用在 Xbox 设备上，以确定主页上特别推荐的活动，例如玩已安装的游戏或从商店购买新游戏，或在 Mixer 上看其他人玩。 自推出个性化体验创建服务以来，Xbox 团队的主要互动指标有了显著提升。

个性化体验创建服务还用于优化必应搜索中的广告投放以及 MSN 新闻中精选的文章，从而再次提高了用户的互动度。

现在你也可以在自己的应用中使用个性化体验创建服务。

### <a name="slide-reinforcement-learning"></a>幻灯片：强化学习

个性化体验创建服务实现一种称作“强化学习”的 AI 技术。 工作原理如下。

[点击声] 假设我们要向用户显示一个主要操作。 [点击声] 用户可能不确定下一步该怎么做，[点击声] 但我们可以显示其中一项建议。 对于游戏应用，[点击声] 那可能是：“玩游戏”、“看视频”或“加入部落”。 [点击声] 根据该用户的历史记录和其他上下文信息（例如，他们的位置、当天时间和星期几），个性化体验创建服务将 [点击声] 排列可能的操作并 [点击声] 对要提升的最佳操作提出建议 [点击声]。 

希望用户会满意 [点击声]，但我们如何确定这一点呢？ 这取决于用户下一步要执行的操作，以及我们是否希望他们这样做。
根据我们的业务逻辑 [点击声]，我们将为接下来执行的操作分配一个“奖励评分”，评分范围为 0 至 1。 例如，花更多的时间玩游戏或阅读文章，或在商店里花更多的钱，可能会获得更高的奖励评分。 [点击声] 个性化体验创建服务将这些信息馈送回排名系统，以便下次我们需要安排某个活动时使用。

### <a name="slide-discovering-patterns-and-causality"></a>幻灯片：发现模式和因果关系

但这不仅仅是一个推荐系统，它有可能向用户展示他们本来就喜欢的内容。 那些用户可能喜欢、但他们自己并不知道的内容呢？ 个性化体验创建服务通常处于利用模式，在该模式下，它会根据历史记录和上下文建议最佳活动，但是有时它也会进入浏览模式，向用户显示他们可能没有看到过的新内容。 这类似于自动化的 A/B 测试系统，但是有两个以上的分支，所有分支都进行了实时测试。

你可以控制“浏览模式”处于激活状态的时间百分比，从而帮助用户发现新内容或功能。

### <a name="slide-personalizer-for-tailwind-traders"></a>幻灯片：适用于 Tailwind Traders 的个性化体验创建服务

在我们的 Tailwind Traders 应用中，对于匿名用户，我们将使用当天时间、星期几以及浏览器 OS 作为“上下文”来影响排名。 对于奖励分数，我们将使用主要部分是否被点击这一信息。 在此代码中，如果用户单击了特别推荐的类别，我们将提供 1 分的奖励分数，否则我们将提供 0 分。

随着时间的推移，个性化体验创建服务将根据当天时间、星期几和 OS 为匿名用户确定最佳精选类别。 它还将在 20% 的时间进行“浏览”，以显示其他本来不会显示的类别。

### <a name="slide-demo-personalizer"></a>幻灯片：演示：个性化体验创建服务

[点击声] 现在让我们在实践中了解个性化体验创建服务。 让我们返回到 Tailwind Traders 主页。 我之前没有提到的是，在这个“推荐”部分，产品部门的顺序是由个性化体验创建服务决定的。
在本例中，它将电气部门显示为主图。 我们刷新网站几次，也可以看到“探索”行为。
显然，个性化体验创建服务目前认为，在每天的这个时候，通过使用我在此处使用的浏览器和操作系统，Garden Center 获得了来自匿名用户的最佳参与度，但最终它会尝试不同的类别 - 此时将弹出管道，个性化体验创建服务将使用它来衡量参与度。

### <a name="slide-pre-built-ai-in-production"></a>幻灯片：生产中的预建 AI

我们已经介绍了一些可用于通过预建 AI 使用类人功能来增强应用程序的方法。 如果计划在面向数百万用户的提供实时功能的生产应用程序中部署这些应用程序，请记住以下我总结的一些注意事项。

### <a name="slide-cost-considerations"></a>幻灯片：成本注意事项

也许你首先要考虑的是：这一切要花多少钱？

[单击] 如果只是想尝试获得开发人员那样的体验，只涉及少量数据、几次尝试，那么通常是免费的。 

[单击] 对于生产量，我们将根据你使用的服务，按量和费率收费。

[单击] 通过此链接，可以获得更多关于定价的详细信息。 在此处可查看关于你的服务和区域的准确定价

如果你是 Azure 的新用户，想体验一下这些服务，可以使用此处的链接注册，并获得 200 美元的额度。

（本幻灯片旨在概述认知服务中的定价模型。 参与者应查看给定链接，了解他们希望使用的服务的确切定价。）

### <a name="slide-data-considerations"></a>幻灯片：数据注意事项

可能还需要考虑数据的去向和使用方式。

你的数据，比如图像或文本，会上传到 Azure 以进行推理，但不会由认知服务存储。 该链接提供了有关隐私和合规性的所有详细信息。 但是，如果你在受管控的行业（例如医药行业）中工作，数据无法通过防火墙，那么还有另一种选择：容器。

### <a name="slide-deployment-with-containers"></a>幻灯片：容器部署

一些认知服务可作为独立容器使用。 只需下载容器映像，将其部署到防火墙后面，然后使用其提供的本地终结点，就像在 Azure 中的操作一样。 区别在于数据永远不会离开你自己的网络。 将容器连接到 Azure 的唯一原因是进行计费 - 使用量的计费方式与 Azure 中完全相同。

### <a name="slide-ethical-considerations"></a>幻灯片：道德注意事项

今天我把最重要的幻灯片留到最后。 你已经了解到将强大的 AI 功能集成到应用程序中是多么容易。 但能力越大，责任越大。了解应用程序将对用户产生的影响，并考虑道德影响，这一点至关重要。

如果你正在使用 AI 技术，则应该在以下道德框架内行事：

* 专注于为用户赋能  ，使其在所从事的工作上取得更大成就，而不是用 AI 取代人类；

* 对所有类型的用户都有包容性  ，使每个人都能平等地通过你的应用程序受益；

* 具有公平性和透明度，特别是不会边缘化未得到充分表示的组。 记住我们先前了解的内容：AI 的好坏取决于训练它所使用的数据，你需要确保你的应用程序适用于所有的潜在用户，不管他们是谁或长什么样。

如果尚未设置道德框架，可以从 Microsoft 自己的人工智能原则开始，可以通过此处的链接阅读详细信息。

### <a name="slide-wrapping-up"></a>幻灯片：总结

可以轻松地通过预建 AI 来添加类人功能。 预建模型不能执行任何操作，但可以快速为你带来长足的发展。 我们稍后将在此学习路径中了解解决剩下 20% 场景需求的自定义模型。

AI 功能强大，但并不是魔术。 它由数据驱动，其核心是非常简单的数学。 请始终将数据纳入考虑因素，并使用它来帮助你了解正在进行的操作。 特别要注意的是，即使是最优秀的 AI 也会出错，尤其是在训练数据中未得到很好表示的组。

最后，试试看！ 无需很多专业知识即可开始使用，但是每个人都需要了解 AI 的道德影响及其对用户的影响，因此请确保已制定使用 AI 的道德框架并遵循。

### <a name="slide-docs-alert"></a>幻灯片：文档提醒

有关 Azure 认知服务的所有详细信息，包括入门指南和参考，请查看 Microsoft Docs。

### <a name="slide-ms-learn-alert"></a>幻灯片：MS Learn 提醒

如果想要学习如何使用认知服务，可以在 Microsoft Learn 上找到免费的课程，这些课程将为你提供分步指导。

### <a name="slide-resources"></a>幻灯片：资源

要查找“文档和学习”以及我在本演讲中提到的所有资源的链接，请查看此幻灯片上的“会议资源”链接。 还可以使用此 github 存储库中提供的代码和脚本来运行我今天提供的演示。 如果你想获得 AI 或数据科学方面的 Microsoft 认证，今天我们面向与会者提供免费认证特殊优惠：有关详细信息，请查看该链接。

谢谢。
