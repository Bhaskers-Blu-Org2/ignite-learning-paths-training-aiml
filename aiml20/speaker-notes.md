# <a name="aiml20-speaker-notes"></a>AIML20:発表者ノート

PPT プレゼンテーション用メモ: [presentations.md](https://github.com/microsoft/ignite-learning-paths-training-aiml/blob/master/aiml20/presentations.md)

関連するデモ スクリプトについては、 https://github.com/microsoft/ignite-learning-paths-training-aiml/tree/master/aiml20 を参照してください。 `DEMO%20Setup.md`で開始します。

## <a name="slide-notes"></a>スライド メモ

スライドはタイトルでのみ識別されます。

### <a name="slide-microsft-ignite-the-tour"></a>スライド:Microsoft Ignite the Tour

プレゼンテーション前の導入スライド

### <a name="slide-using-pre-built-ai-to-solve-business-problems"></a>スライド:事前構築済み AI を使用してビジネスの課題を解決する

ようこそ。私は、\<所属の名前\>と申します。 本日は、膨大なコンピューティング能力やAIの専門知識を必要とせずに、アプリケーションに人工知能の機能を追加する方法についてご説明いたします。
これを実現するには、クラウドで利用できる事前に構築された各種の AI サービスを使用します。特定のニーズに合わせてこれらのカスタマイズも行います。

(スライド上の担当者情報を差し替えてください。)

### <a name="slide-resources"></a>スライド:リソース

本日の講義では、情報やリソースへのリンクを多数ご紹介いたしますが、プレゼンテーション中にこれらのリンクが見つからなくてもご心配は要りません。詳細な情報はすべて、ここに示すセッション リソース ハブに含まれています。 また、デモもご用意しております。この Github リポジトリには、ワンクリックで Azure にすべてをデプロイできるボタンをはじめとする、ソース コード一式が取り揃えられており、すべてご自分でお試しいただけます。 このスライドはプレゼンテーションの最後にも表示致しますので、ぜひカメラをご用意いただければと思います。

### <a name="slide-adding-humanlike-capabilities-to-apps"></a>スライド:人間的な機能をアプリに追加

ここでは、事前構築された AI サービスを使用してアプリケーションに人間的な機能を追加しますが、これはどうことなのでしょうか？ 次に例をいくつか示します。

### <a name="slide-enhance-apps-with-humanlike-capabilities"></a>スライド:人間的な機能でアプリを強化

[クリック] チャット インターフェイスを追加するなどして、アプリに音声機能を追加します。
 
[クリック] アプリに画像の内容を理解する視覚機能を追加します。

[クリック] アプリにユーザーが何をしたいと思っているかを理解する直感を与え、ユーザー インターフェースを自動的に最適化することができます。

[クリック] アプリに理解力を与え、言語に関係なく、ユーザーとコミュニケーションさせることができます。

[クリック] あるいは、データ ストリームをスキャンして異常を検出し、それに応じて対応するという人間によるプロセスを自動化することもできます。

これらは、ほんの一例です。

### <a name="slide-overview-of-azure-cognitive-services"></a>スライド:Azure Cognitive Services の概要

ここまで、AI 技術がどのように役立つかについて、多くお話ししてきました。 しかし、実装するには大量のデータと多くの技術的な専門知識が必要だとお考えではありませんか？

答えは、いいえです。 Microsoft Research の専門知識を活用することができます。その豊富なデータ リポジトリと AI の専門知識を活かして、シンプルな REST API による呼び出しが可能になった既製の AIサービスが Azureに作成されました。 

それが、Azure Cognitive Services です。

### <a name="slide-azure-cognitive-services"></a>スライド:Azure Cognitive Services

Azure Cognitive Servicesには 数にして 20 以上の API がありますが、より大きなカテゴリで分類すると、人間が持つ次のような能力にまつわる機能を提供しています。

* 視覚:写真、描画、テキストや手書き、ビデオの内容を理解

* 音声:音声を理解して認識し、自然で人間的な音声を生成するツール。

* 言語:書かれたドキュメントやテキストの内容を理解し、人間の言語間で翻訳。

* 決定: Azure Cognitive Services のまったく新しいカテゴリであり、データ、コンテンツ、アプリケーション ユーザー インターフェイスに関して人間的な選択を行う。

* 検索: 大規模な非構造化リポジトリのコンテンツに関する、自然言語による質問に回答します。 

### <a name="slide-azure-cognitive-services-with-service-names"></a>スライド:Azure Cognitive Services (サービス名を含む)

"検索" カテゴリは、このラーニング パスの前回の講義である AIML10 で説明しました。 この講義では、その他の利用可能なサービスのいくつかに触れ、それらを使用して小売業者の Web サイトを強化します。[クリック]

Computer Vision: 商品の写真の内容を分析するために使用します。

Custom Vision: 小売業者が販売する製品を特定するために使用します。

Personalizer: お客様の好みを観察することによって Web サイトのレイアウトを自動的に調整し、匿名ユーザーであっても最初に最適な製品カテゴリを提示します。

ただし、Cognitive Services の設定と使用の原則はすべての API に共通しているため、ここで学習する内容はどの AI サービスを使用したい場合でも応用できます。

### <a name="slide-computer-vision"></a>スライド:Computer Vision

まず、Computer Vision 用に事前構築された AI について見てみましょう。そして、これを使用してアプリケーションに視覚機能を追加し、特定のニーズに合わせてカスタマイズする方法について見てみましょう。

### <a name="slide-shop-by-photo"></a>スライド:Shop By Photo

これは、ハードウェアの小売業者である Tailwind Traders の Web サイトです。 Tailwind Traders の Web サイトには、製品カタログの参照、製品のオンライン注文、小売店で取り扱っている製品の検索機能など、多くのよくある e コマース機能があります。
しかし、これから説明するように、AI 対応の機能もいくつかあります。

ご想像のとおり、Tailwind Traders は架空の会社なので、アプリを皆さんがデプロイするために必要なすべてのソース コードをご提供することができます。 ソース コードについては、このスライドの下部にあるリンクを参照してください。

### <a name="slide-demo-shop-by-photo"></a>スライド:デモ:Shop By Photo

デモ:"問題の定義:Shop by Photo が壊れています"

Tailwind Traders のライブ Web サイトにアクセスしてみましょう。 [クリック]

AI 対応の機能の 1 つに、"Shop by Photo" と呼ばれるものがあります。 この機能を使用すると、顧客は購入したい製品の写真をアップロードできます。そして、その製品が購入可能かどうかをアプリに教えてもらうことができます。 さあ、試してみましょう。 まず、自分が興味のあるドリルの画像をアップロードします。すると Tailwind Traders アプリではその画像を分析してドリルであることを認識し、Tailwind Traders が販売しているドリルを表示して、それがストア内のどこにあるかを示します。

では、別の画像で試してみましょう。 ホームページに戻り、"Shop by Photo" 機能をもう一度使用します。今回は、ペンチの画像を選びます。 残念ながら、アプリでその画像を分析すると、ハンマーであると認識します。 これは明らかにうまく機能していません。それでは次に、何がよくなかったのかを分析し、Computer Vision を使用して修正することができるかどうか見てみましょう。 

### <a name="slide-how-computer-vision-works"></a>スライド:Computer Vision のしくみ

しかしその前に、Computer Vision がどのように機能するかを理解するために、理論を少し掘り下げてみることにしましょう。 心配する必要はありません。数学はほとんどありません。Computer Vision のしくみについて少し理解しておくと、問題点とその解決方法を理解するのに役立ちます。

### <a name="slide-tasks-xkcd-comic"></a>スライド:タスク (XKCD コミック)

(10 秒間一時停止)

少し前までは、コンピューターに写真を渡したり、画像に写っているものについて有益な情報を得たりすることは 文字通りサイエンス フィクションにすぎませんでした。 この XKCD は 2014 年 9 月に公開されました。 それから 5 年経った今、コンピューターは、ビッグデータ、GPU コンピューティング、畳み込みニューラル ネットワークの登場によって、写真に写っているのが鳥であるかどうかを簡単に見分けることができるようになりました。 その方法を見てみましょう。

### <a name="slide-how-neural-networks-work-brandon-rohrer"></a>スライド:ニューラル ネットワークのしくみ (Brandon Rohrer)

この説明は、Brandon Rohrer氏の許可を得て改変しています。彼は、AI と機械学習を様々な側面から詳しく解説する優れたブログと動画のチュートリアル シリーズを管理しています。 今回はその概要のみに触れています。詳細については、Brandon のブログを参照してください。

### <a name="slide-computer-vision--convolutional-neural-network"></a>スライド:Computer Vision / 畳み込みニューラル ネットワーク

AI が "ディープ ラーニング" と呼ばれるもので動いていることをお聞きになったことがあるかもしれません。
「ディープラーニング」の「ディープ」は、「深い」という意味ではなく、画像を分析する際に、多くのレイヤーからなるニューラル ネットワークを通過することを単に意味しています。 これで終了です。

画面には、非常にシンプルなニューラル ネットワークが表示されています。 これにはレイヤーが 5 枚しかありませんが、現実の視覚システムには数十枚、おそらく数百枚のレイヤーがあります。 これは、画像を入力として取得し、その画像を 4 つのオブジェクト (犬、自転車、りんご、テニス ボール) のいずれかに分類するように設計されています。 つまり、認識するようにトレーニングされたオブジェクト以外のものを検出することはできません。

### <a name="slide-trained-convolutional-nn"></a>スライド:トレーニングされた畳み込み NN

ニューラル ネットワークをトレーニングすると、ネットワーク レイヤーを 1 枚づつ通して画像を異なる小さな画像に変換しつつ、入力画像が渡されます。 各レイヤ-は、前のレイヤーで生成されたイメージを再結合し、画像は徐々に小さくなって、最終的には 0 から 1 の値を持つ 1 つのピクセルになります。 この値は、指定されたオブジェクトを表す画像に対するニューラル ネットワークの信頼度を表します。数値が大きいほど、信頼性が高くなります。

この例では、自転車の画像を入力し、右側の "自転車" ノードが最も高い値を示していることがわかります。 このニューラル ネットワークは、自転車 (少なくともこの特定の自転車) を検出できるよう十分にトレーニングされていることがわかります。 では、どのようにニューラル ネットワークが "トレーニング" され、その過程で画像はどのように変換されるのでしょうか。

ネットワークの各ノード (各円) では、画像にフィルターが適用されます。
これは、スナップショット フィルターや Instagram フィルターのようなものですが、画像をセピア調にしたり、顔にウサギの耳を付ける代わりに、トレーニング プロセスで定義した別の処理を行います。 これについて、詳しく見ていきましょう。

### <a name="slide-filters-1"></a>スライド:フィルター(1)

単純な画像を考えてみましょう。 これは、バツ印の画像です。 大きさは 9x9 ピクセル、白は "+1"、黒は "-1" です。 ニューラル ネットワーク内の各ノードで行われているように、この画像にフィルターを適用します。

### <a name="slide-filters-2"></a>スライド:フィルター (2)

この画像を変換するには、3x3 のグリッドの重みを適用します。 このような小さいグリッドは、コンピューター ビジョン システムでよく使用されます。 このグリッドでは、-1 と 1 の重みだけが使用されますが、通常は、その範囲の乱数のように表示されます。 [クリック]

画像に重みを適用するには、画像内の特定のピクセルに中央揃えで重みのグリッドを重ねます。 [クリック] 次に、各ピクセル値ごとに重みを掛け、その平均値を取得します。 この平均値は、出力画像に対応するピクセルになり、重みグリッドの中央のピクセルに位置合わせされます。

入力画像の端を中心となるピクセルとして使用できないことにお気づきかもしれませんが、これは出力画像は 2 行と 2 列分、入力より小さくなるためです。 これが (他の種類の変換も踏まえて)、レイヤーを下に移動するにつれて (サイズが 1 ピクセルになるまで) 画像のサイズが小さくなる理由です。

### <a name="slide-filters-3"></a>スライド:フィルター(3)

重みグリッドを下と右にそれぞれ 2 ピクセル移動してみましょう。 重みにソース ピクセルを掛けて平均値を取得すると、異なる出力ピクセル (.55) が得られます。 ニューラル ネットワークは、ソース 画像の行と列の重みをスイープして、出力画像のピクセルを作成します。

ちなみに、画像全体でフィルターをスイープするプロセスは、シンプルな数学演算でありながら、「畳み込み」という複雑な名前が付けられています。 これが、畳み込みニューラル ネットワークと呼ばれる理由です。

### <a name="slide-training-an-image-classifier"></a>スライド:画像分類器のトレーニング

さて、これでニューラル ネットワーク内の各ノード (円) は、入力画像が変換されたものであり、重みのグリッドによって決定されることがわかりました。 ニューラル ネットワークをトレーニングする秘訣は、正しい数値が最終的に得られるような重みを選択することです。

[クリック] これを行うには、犬、自転車、りんご、テニス ボールの画像が多数含まれているトレーニング データを使用します。 私たちは各画像が表す内容を理解できる (人間がそれを見て、ラベルを付け、または "注釈" を付けている) ため、正しいノードが各ケースで最大の値を取得できるように、または少なくともできるだけ頻繁に取得できるよう、重みを選択します。

ただし、実際の視覚 ネットワークでは、重みの数は数百万にもおよび、計算対象となるラベル付けされた画像の数は数百万にもなる可能性があります。 重みはどのように決定しますか。

### <a name="slide-learning-backpropagation"></a>スライド:学習:逆伝搬法

機械学習に関する本の多くがこの時点数学に踏み込み、「逆伝搬法」や「学習率」、「コスト関数」といった話を持ち出しはじめます。 しかし、AI エンジニアでもない限り、これらすべてを無視することができます。その理由は 2 つあります。

第一に、ビッグデータ ストアや GPU プロセッサなどの強力なコンピューティング リソースを活用しながら、こうした計算をすべて代行できる優れたツールが数多く提供されていることです。 これを行う Tensorflow や Pytorch のようなツールを耳にしたことがある方もいらっしゃるかと思いますが、これらについては、このラーニング パスの後半でも説明します。 

第二に、とはいえこれらのツールをすべて活用するには、大量のトレーニング データ、強力なコンピューティング リソース、さらに AI エンジニアのチームが必要です。 その代わりに、大量のデータ、コンピューティング、専門知識をすでに使用しているプロジェクトまたは企業のリソースを使用してニューラル ネットワークのトレーニングを行い、それを API 経由で使用することができます。

### <a name="slide-pre-trained-convolutional-nn"></a>スライド:事前にトレーニングされた畳み込み NN

そのため、事前に定義された重みを持つモデルを使用するだけで済み、モデルでトレーニングされているオブジェクトの分類を検出するだけでよい場合は、準備はこれで完了です。 画像を提供するだけで、事前にトレーニングされたモデルによって生成される分類を使用できます。 

モデルによっては、単に分類するだけでなく、オブジェクトの位置情報を検出したり、他の方法で画像を分析したりすることができます。

### <a name="slide-demo-cognitive-services-computer-vision"></a>スライド:デモ:Cognitive Services Computer Vision

次に、事前にトレーニングされた AI モデルのCognitive Services Computer Vision を試してみましょう。 このサービスでは、指定された画像が分析され、検出されたオブジェクトのタグ (または分類) が提供されます。 これらは従来からある畳み込みニューラル ネットワークの右側にある信頼度が高いスコアに関連付けられるラベルにすぎませんが、ここでは数千ものオブジェクトを識別できる Microsoft の強力なニューラル ネットワークが使用されています。

aka.ms/try-computervision でお試しいただける Web ベースの UI をご用意しているため、試してみましょう。 後ほど、プログラムを使用して API にアクセスする方法についても説明します。

### <a name="video-computer-vision-via-web"></a>ビデオ:Web を使用した Computer Vision

[クリック] これは Cognitive Services Computer Vision のページです。 このページで、少し下にスクロールすると、Web ベースのフォームがあります。これを使用して Ｗeb から、またはローカル ファイルとして保存されている分析用の画像をアップロードできます。 では、安全帽を被っている男性の写真をアップロードしてみましょう。 ほんの数秒で、Computer Vision サービスからその画像の分析が返されます。 左側には画像内で検出されたオブジェクト、右側には詳細な分析を含む JSON 形式の出力が表示されます。 これには、画像内で検出されたオブジェクトの名前と場所、画像に関連付けられているタグまたはラベルの一覧、画像についての平易な説明 (この場合は "ヘルメットを被っている男性")、およびその他の有用な情報が含まれます。

### <a name="slide-cognitive-services-computer-vision"></a>スライド:Cognitive Services Computer Vision

出力の [オブジェクト] セクションで、画像内で 2 つのオブジェクト (帽子の類と人間) が検出されていることを確認できます。

今回は、画像全体の分類と信頼度スコアを提供する [タグ] セクションに注目します。 この場合、"man" の次に信頼度が高い分類は "headdress" になりますが、これは、Shop by Photo アプリで必要な "hard hat" とは異なります。
残念ながら、この API は、ヘルメットは検出できますが安全帽を検出できるようにトレーニングされていません。ヘルメットもここでは分類の信頼度が 6 番目となっています。 この問題を解決する方法については、後程説明します。

ただし、視覚機能をアプリケーションに組み込む場合は、Web フォームを使用するのではなく、プログラムを使用して Computer Vision API にアクセスできます。
その方法を見てみましょう。

### <a name="video-computer-vision-via-cli"></a>ビデオ:CLI を使用した Computer Vision

HTTP エンドポイントに接続できるものであれば、どの言語を使用しても Cognitive Service API へのインターフェイスを設定することができますが、ここでは、[クリック] Azure CLI を使用してリソースを作成し、"curl" を使用して Computer Vision API に接続する bash スクリプトを使用します。 ローカル シェルに Azure CLI をインストールすることもできますが、ここでは Visual Studio Code の "Azure Account" 拡張機能を使用して Cloud Shell を起動します。つまり、何もインストールする必要はありません。 シェルの準備ができたら、この bash スクリプトから直接コマンドを実行できます。 

最初のコマンドで、リソース グループを作成します。これは、API を認証するために必要なキーを保持するために使用します。

次の手順で、キーを作成します。 ここでは、Computer Vision を含む多くのサービスで使用できるオムニバスの Cognitive Services キーを作成しています。

それから、このキーをターミナルに直接表示できます。 [待機] これらのキーのどちらを使用しても API へのインターフェイスを設定することができます。ここでは、最初のものを環境変数に保存します。

このキーを使用すると、Computer Vision サービスによって提供されるエンドポイント URL に接続できます。その URL も同様に環境変数に保存しましょう。

次に、分析する画像を選択できます。 ここでは、画像の URL を指定します。しばらく前に見たものと同じ、安全帽を被った男性の画像です。

ここで、curl を使用して JSON 入力を渡すことによって、キーと画像 URL をエンドポイントに渡すことができます。 数ミリ秒で、JSON として画像分析が返されます。 前に Web インターフェイスで見たものと同じ出力が表示されます。

もちろん、どれでも好きな画像を使って行うことができます。 別の画像 (この場合はドリルの画像) でもう一度試してみましょう。 ここでも、curl を使用してこれを API に渡すことができます。 [待機] 興味深いことに、この画像に関連付けられている最上位のタグは "camera" で、残念ながら実際の工具を検索するのには役立ちません。"drill" が必要です。

### <a name="slide-adapting-computer-vision-models-with-your-own-data"></a>スライド:Computer Vision モデルを自身のデータに適合させる

これで、Tailwind Traders の Shop by Photo 機能で Computer Vision API が最適な選択肢ではない理由がお分かりいただけたかと思います。 あるケースでは、Tailwind Traders が使用している視覚モデルは、同社が販売している特定の製品を識別するためのトレーニングを受けていません。 また別のケースでは、検出するようにトレーニングされているオブジェクトの種類が*あまりに多い*ため、間違ったオブジェクトが検出されています。 今ご覧になったように、ドリルの写真を提示したにも関わらず、Tailwind Traders 社が販売していない製品である "カメラ" のタグが返されました。

幸い、この問題は解決することができます。 では、少し理論について説明しましょう。

何千もの画像を識別するために事前トレーニングされている視覚モデルから始めて、そのオブジェクトが元のモデルのトレーニング データの一部ではなかったとしても、目的のオブジェクトだけを識別するように調整する方法があるとしたらどうでしょう。 奇妙なことのようにも思えますが、どのように動作するかを見てみましょう。

### <a name="slide-transfer-learning"></a>スライド:転移学習

ここでは、前にも使用した、トレーニングされた畳み込みニューラル ネットワークを用意していますが、これとは異なり、オブジェクトの分類を行う最後のレイヤーは削除されています。 残っているのは、最後から 2 番目のレイヤーから取得した画像です。 画像 (たとえば、3x3 の画像) であることを無視して、これをデータとして捉えます。 信頼スコアを取得する代わりに画像を左側にフィードすると、それぞれが 9 つのデータ ポイントを持つ配列のコレクション、つまり "特徴" が得られます。 このトイ ネットワークでは、F1、F2 のように F8 までラベル付けされています。
左側に配置する各画像から、右側に異なる特徴のコレクションを生成します。

これらの特徴が*どのようなもの*なのかはよくわかっていませんが、ニューラル ネットワークが*もともと*トレーニングされていた画像の種類を分類するのに役立ったことは確かです。 ある特徴は 「緑っぽさ」を表しており、木やテニスボールを分類するのに役立ったのかもしれません。
別のものは、画像内の円形の領域の数をカウントし、自転車と信号機を分類するのに役立ったようです。 重要なのは、これらの特徴が事前に定義されたわけではないということです。これらはトレーニング データから_学習_されたものであり、_一般的な_画像の分類に役立ちます。 

つまりは、これらの特徴を使用して、元のネットワークでトレーニングされていないオブジェクトを分類することができるのです。

### <a name="slide-transfer-learning-training-1---with-the-hammer"></a>スライド:転移学習のトレーニング (1 - ハンマー)

ハンマーと安全帽を識別する新しいモデルが必要だとします。 左にハンマーの画像を渡し、右側でその特徴を収集します。 この例では、8 つのデータ ベクトル (特徴ごとに 1 つ) と、オブジェクト型のバイナリ標識を取得します。 様々な複数のハンマーの画像に対してこの手順を繰り返し、データ ベクトルとバイナリ標識を毎回収集します。

### <a name="slide-transfer-learning-training-2---with-the-white-hard-hat"></a>スライド:転移学習のトレーニング (2 - 白い安全帽)

次に、安全帽の画像で同じ操作を行います。 今回も、各回ごとに 8 つのデータ ベクトルと各画像のバイナリ標識を収集します。

すべてをまとめると、できあがったのは それぞれにバイナリの結果が関連付けられたデータ ベクトルのコレクションです。 データ サイエンスに触れたことがある方は、次に何が起こるかお分かりいただけるかと思いますが、ロジスティック回帰や 1 レイヤーからなるニューラル ネットワークなどの単純な予測モデルを構築して、特徴から新しいオブジェクトの分類を予測することができます。

### <a name="slide-transfer-learning-trained-model"></a>スライド:転移学習トレーニングされたモデル

驚くほどうまく機能していることがわかります。 大量のデータが必要なわけではありません。予測するカテゴリがかなり明確であれば、多くの場合、数十枚の画像で間に合います。 また、比較的少量のデータから 100 個ほどのバイナリ結果を予測するだけなので、多くのコンピューティング能力は必要ありません。

もちろん、これはトイモデルによる例です。通常は 3 つ以上のオブジェクトを識別する必要があり、最後から 2 番目のレイヤーで基になるニューラル ネットワークによって生成される特徴の数は、8 より多くなることは確かです。 しかし、少量の新規データとわずかなコンピューティング能力で実現でき、多くの場合、非常にうまく機能するというこの原則は変わりません。

### <a name="slide-microsoft-cognitive-services-custom-vision"></a>スライド:Microsoft Cognitive Services Custom Vision

もちろん、転移学習モデルを自分でトレーニングする必要はありません。 Cognitive Services Computer Vision の高度な視覚モデルをベースとして使用し、独自の画像と分類を Custom Vision と呼ばれるサービスに提供できます。

Computer Vision と同様に、API を使用してプログラムによる学習モデルのトレーニングを行うこともできますが、Custom Vision では、モデルをトレーニングするための便利な Web UI も提供されています。 ここでは、Tailwind Traders の Shop by Photo 機能のモデルをトレーニングするために使用してみましょう。

### <a name="slide-demo-customized-object-recognition"></a>スライド:デモ:カスタマイズされたオブジェクト認識

デモの手順: https://github.com/microsoft/ignite-learning-paths-training-aiml/blob/master/aiml20/DEMO%20Custom%20Vision.md

### <a name="video-customvisionai"></a>ビデオ: customvision.ai

[クリック] これが、Custom Vision の Web ベースのインターフェイスです。 優れた UI を使用して、転移学習分析用の新しい画像を提供できます。 ご存知のとおり、このプロジェクトには既に多数の画像がアップロードされています。 ドライバー、ペンチ、ドリル、ハンマーの写真をアップロードしました。これらをカスタム モデルのトレーニングに使用します。 さらに、Tailwind Traders が販売している製品をもう 1 つ検出したいと思います。安全帽です。 ここで、[Add images]\(画像の追加\) をクリックし、ハード ドライブ上のフォルダーを参照します。ここには安全帽の写真が何枚か収集されています。それらをすべて選択してサービスに追加し、トレーニングで使用する "hard hat" というラベルを指定します。

これらのファイルをアップロードするには、しばらく時間がかかります。その間にご覧いただきたいのは、このプロジェクトにそれほど多くの画像が含まれていないことです: 約 180 枚、または 5 つのカテゴリそれぞれに対して 2, 3 ダース程度です。 場合によっては、もっと少数です。 それでも、この 5 つのオブジェクトの種類がかなり異なるため、このモデルは非常にうまく機能します。

では、[Train]\(トレーニング\) ボタンをクリックして、転移学習を開始しましょう。 [Quick Training]\(クイック トレーニング\) を選択します。 現在、複雑な視覚モデルに対してこれらすべての画像を処理し、転移学習を使用してこの 5 つのカテゴリに対する予測モデルを作成しています。 数秒しかかかりません。非常に優れたモデルです。
確率のしきい値には限界が設定されています。それを下回ると、分類をまったく予測しません。 信頼度が 50% 以上の分類のみを受け入れる場合、その予測の 90.9% は正しいものになります。これが "精度" です。 そして、このモデルでは画像全体の 88.2% が正しく分類されます。これは "再現率" です。 アプリでは、誤った判断を下すことを許容するか、まったく判断しないかを考慮してしきい値を選択します。 Tailwind Traders の場合、顧客に誤った製品を提案してもたいした問題ではないので、低い方にしきい値を設定できます。 もしこれががんを検出するアプリであれば、おそらく別の判断をするでしょう。

次に、まったく処理したことのない新しい画像でこのモデルを試してみましょう。 これを行うには、Quick Test\(クイックテスト ボタンをクリックします。 "test images" フォルダーから新しいファイルをアップロードします。 "安全帽を被っている男性" を試してみましょう。 すると、その予測がまさに "hard hat" であり、確率が 99.9% であることがわかります。おそらく、どのようなしきい値を選択しても、同様の判断を下すことができるでしょう。

別の画像 (ドリル) を試してみましょう。 このモデルでは、この画像が確率 94.5% のドリルとして識別されます。 最後に、ペンチの画像を試してみましょう。これは 99.9% の信頼度で識別されます。

したがって、このモデルは 200 枚以下の画像でトレーニングされたにも関わらず適切に動作しています。
これは、潜在的なラベルを Tailwind Traders で販売されている製品のみに制限したためです。

これで満足のいくモデルができましたので、エクスポートしてアプリに組み込むことができます。 [Export]\(エクスポート\) ボタンをクリックすると、iOS または Android 用のモデルをコンテナーとしてエクスポートしたり、この場合のようにユニバーサルな ONNX 形式でエクスポートしたりできます。 これで、このモデルをハード ドライブにダウンロードしました。

### <a name="slide-portable-deep-learning-models"></a>スライド:移植可能なディープ ラーニング モデル

カスタム モデルを ONNX 形式でエクスポートしました。

ONNX、または Open Neural Network Exchange は、AI モデルの無料交換やデプロイを促進するために Microsoft と Facebook が立ち上げたオープンな標準フォーマットであり、幅広いアプリケーションやテクノロジ ベンダーにサポートされています。

これでカスタム ビジョン モデルのトレーニングが完了したので、Tailwind Traders アプリに統合してみましょう。 統合するには、ONNX 形式のモデルから予測を生成する関数を提供するオープンソースの推論エンジンである ONNX Runtime を使用します。

### <a name="slide-onnximagesearchtermpredictorcs"></a>スライド:OnnxImageSearchTermPredictor.cs

カスタム モデルを作成したので、API を使用してアプリで呼び出すことができます。 ここでは、生成した ONNX ファイルから新しい "推論セッション" を作成し、アップロードした画像から文字列として分類ラベルを生成します。
次に、これを Tailwind Traders の Web サイトの既存の検索機能に渡し、結果を表示します。

```csharp 
var session = new InferenceSession(filePath);

...

var output = session.Run(new[] { input });
var prediction = output
    .First(i => i.Name == "classLabel")
    .AsEnumerable<string>()
    .First();
```

### <a name="slide-demo-onnx"></a>スライド:デモ:ONNX

デモ:ONNX のデプロイ

### <a name="video-kudu"></a>ビデオ:Kudu

[クリック] Custom Vision から先ほどエクスポートしたモデルは実際には ZIP ファイルです。これには、実際の ONNX ファイルであり先ほど作成したニューラル ネットワークのテキスト表現である model.onnx と、マニフェスト ファイルが含まれています。 

既存の Tailwind Traders Web サイトでは、products.onnx と呼ばれる ONNX ファイルで表現されるコンピューター ビジョン モデルが既に使用されています。 問題は、Tailwind Traders で販売されている製品の多くがこのモデルによって正しく認識されないことです。 したがって、Custom Vision からエクスポートしたばかりの model.onnx ファイルを products.onnx という名前に変更して、この Web アプリ内のそのファイルを置き換えます。これにより、トレーニングした 5 つの製品すべてが Shop by Photo によって認識されるようになります。

この Azure portal では、Tailwind Traders Web サイトを実行する App Service リソースを確認できます。 この App Service 内で、[Development Tools]\(開発ツール\) セクションの、[Advanced Tools]\(高度なツール\) 機能を選択します。 これにより、Kudu インターフェイスが起動します。 起動したら、デバッグ コンソールを使用してこの Web サイトのファイルシステムを参照できます。 [site]、[w-root]、[Standalone]、[OnnxModels] を参照します。ここに products.onnx ファイルが配置されています。 ここで、Custom Vision で作成した新しいバージョンの products.onnx ファイルに置き換えることができます。

App Service に戻って、Web サーバーを再起動します。これにより、Shop by Photo 機能で新しい ONNX モデルが使用されるようになります。

### <a name="video-netron"></a>ビデオ:Netron

[クリック]  Web サイトが再起動されるのを待っている間に、インストールしたばかりの ONNX モデルの内容を見てみましょう。 Lutz Roeder 氏によって開発された、Netron と呼ばれる小さなすばらしい Web アプリがあります。これを使って、ONNX ファイル内のニューラル ネットワークを調べることができます。 では、その products.onnx ファイルを開いてみましょう。 ここでは、このモデルによって表されるニューラル ネットワークの実際のレイヤーを確認できます。 少し拡大して、上部にある入力を見てみましょう。 入力は画像です。 これは、サイズが 224 x 224 ピクセルの 3 枚のレイヤーからなる RGB 画像です。 実際には、ONNX runtime に提供する前に、ユーザーが提供した画像をトリミングしてスケールダウンする必要がありました。 これは、コンピューター ビジョン システムの視力があまりよくない、つまり解像度がかなり低い画像で動作しているにもかかわらず、非常にうまく機能しているということです。

次に、縮小してネットワークをスクロールしてみましょう。 Custom Vision で作成されたニューラル ネットワーク内のすべてのレイヤーを見ることができます。この講義の前半で説明したように、各レイヤーは入力画像を変換し、フィルターを適用し、出力イメージを再結合します。 しかし、最終的に出力レイヤーに到達すると、出力が 5 つの値のリストであることがわかります。これは、トレーニングした 5 つの製品 (ハンマー、安全帽など) を示します。このモデルが各カテゴリに対して予測する信頼度を示す "loss" というラベルの付いた値もあります。 アプリでは、信頼度の高さについての要件に合わせて独自のしきい値を選択します。

Tailwind Traders Web サイトが再起動されたので、ホームページに戻り、新しい視覚モデルがどのように機能するかを確認しましょう。 それでは、写真をアップロードして、テスト画像の 1 つをもう一度試してみましょう。具体的には、これまでにうまく機能しなかったペンチの画像です。 Web サイトではこれがハンマーであるとは考えず、確かに "ペンチ" を検索し、売り出されているすべての製品を表示しているのがわかります。

### <a name="slide-optimizing-app-ui-with-cognitive-services-personalizer"></a>スライド:Cognitive Services Personalizer によるアプリ UI の最適化

残りの時間を使って、Cognitive Services の "決定カテゴリ" から、事前構築された AI の例をもう一つご紹介しようと思います。 この "Personalizer" サービスを使用すると、ユーザーが実行しようとしている可能性が最も高いものと、*提供側がユーザーにやってほしい事*を天秤にかけ、アプリのインターフェイスをリアルタイムでカスタマイズすることができます。

### <a name="slide-recommended-screenshot"></a>スライド:推奨 (スクリーンショット)

これがどのように機能するかについては、Tailwind Traders Web サイトの [おすすめ] セクションで見ることができます。 この図は、ストアで利用できる一連の部門を示しています。1 つは大きな "ヒーロー" の画像で、いくつかの小さな画像と組み合わされています。

Personalizer サービスは、"強化学習" と呼ばれる AI 手法に従って、これらのセクションがどのように表示されるかをユーザーに代わって選択します。

### <a name="slide-personalizer-in-action"></a>スライド:Personalizer 稼働中

Microsoft では、長年にわたって Personalizer の開発を行ってきました。 これは、Xbox デバイスで使用されており、インストールされているゲームをプレイする、新しいゲームをストアから購入する、Mixer を通じて他のユーザーのゲームプレイを視聴するなど、ホーム ページでおすすめするアクティビティを決定しています。 Personalizer が導入されたことにより、Xbox チームでは主要なエンゲージメント メトリックの上昇を目の当たりにしています。

Personalizer はまた、Bing 検索の広告の配置の最適化や、MSN ニュースで取り上げられる記事にも使用され、ユーザー エンゲージメントの向上において、優れた結果を残しています。

今では、自分のアプリでも Personalizer を使用できるようになりました。

### <a name="slide-reinforcement-learning"></a>スライド:強化学習

Personalizer は強化学習と呼ばれる AI 手法を実装しています。 しくみは次のとおりです。

[クリック] ユーザーに "ヒーロー" アクションを表示するとします。 [クリック] ユーザーは次に何をするか決めていないかもしれません [クリック] が、いくつかの候補のうちのひとつを私たちが提案できるかもしれません。 ゲームアプリの場合は、[クリック] "ゲームの再生"、"映画を見る"、または "クランに参加" などでしょう。 [クリック] そのユーザーの履歴やその他のコンテキスト情報 (たとえば、場所、時刻、曜日など) に基づいて、Personalizer サービスでは、[クリック] 考えられるアクションをランク付けし、[クリック] 最も良いものを提案します。 

ユーザーが満足することが望ましいですが [クリック] 、どうすればそれが確実にわかるでしょうか。 それは、このユーザーが次に行うことと、それが私たちの提案と一致したかどうかによって判断できます。
ビジネス ロジックに従って、[クリック] 次に起こることに対して 0 から 1 の "報酬スコア" を割り当てます。 たとえば、ゲームをもっと長くプレイしたり、記事を読んだり、ストアでもっとお金を使ったりすることが、より高い報酬スコアにつながるかもしれません。 [クリック] Personalizer では、次にアクティビティを特集する必要があるときのために、その情報を元のランク付けシステムにフィードします。

### <a name="slide-discovering-patterns-and-causality"></a>スライド:パターンと因果関係の探索

しかし、これはユーザーにすでに気に入っているものを提示する恐れのある、単なるレコメンダー システムではありません。 ユーザーが求めているが、求めていることを認識していないものは何でしょうか。 通常、Personalizer は履歴とコンテキストに基づいて最適なアクティビティを推奨するエクスプロイト モードですが、時に探索モードに入り、通常では目にすることのない新しいものをユーザーに提示します。 これは自動化された A/B テスト システムに似ていますが、3 つ以上の分岐があり、すべてリアル タイムでテストされます。

探索モードがアクティブ化される割合を制御し、ユーザーが新しいコンテンツまたは機能を発見できるようにします。

### <a name="slide-personalizer-for-tailwind-traders"></a>スライド:Tailwind Traders 用の Personalizer

Tailwind Traders アプリでは、匿名ユーザー向けに、時間、曜日、ブラウザーの OS を "コンテキスト" として使用して順位付けに影響を及ぼします。 報酬スコアは、ヒーローのセクションがクリックされたかどうかに関わらず使用します。 このコードでは、おすすめカテゴリがクリックされた場合は 1、それ以外の場合は 0 の報酬スコアを提供します。

時間の経過に応じて、Personalizer は時間、曜日、OS に基づいて匿名ユーザーに最適なカテゴリを決定します。 また、時間の 20% を "探索" の時間に充て、埋もれているカテゴリーを発掘します。

### <a name="slide-demo-personalizer"></a>スライド:デモ:Personalizer

[クリック] 次に、動作中の Personalizer を見てみましょう。 Tailwind Traders ホームページに戻ります。 まだ説明していませんでしたが、この [おすすめ] セクションでは、製品部門の順序が Personalizer によって決定されます。
この例では、電気部門がヒーローのイメージとして表示されています。 WWeb サイトを何回か更新すると、"探索" 動作を確認することもできます。
現在 Personalizer では明らかに、ここで使用しているブラウザーとオペレーティング システムを 1 日のこの時間に使用する匿名のユーザーから最適なエンゲージメントを得るのは Garden Center であると考えています。しかし最終的にはさまざまなカテゴリを試します。ここでは給排水設備がポップアップ表示されました。Personalizer ではこれも使用してエンゲージメントを測定します。

### <a name="slide-pre-built-ai-in-production"></a>スライド:運用環境における事前構築済み AI

事前構築済み AI を使用して、人間的な機能を搭載でアプリケーションを強化する方法をいくつか見てきました。 最後に、これらのアプリケーションを実稼働アプリ、例えば百万ものユーザー向けのリアルタイムな機能を含むアプリに展開する予定がある場合は、いくつかの点に注意してください。

### <a name="slide-cost-considerations"></a>スライド:コストに関する考慮事項

まず最初に考えるべきは、最終的にかかるコストかと思います。

[クリック] "開発者のように" あらゆるものを試す場合、データ量はごくわずかであり、いくつかのことを試すだけであれば、通常は無料です。 

[クリック] 実稼働のボリュームの場合、使用しているサービスに応じて、ボリュームと料金に応じて課金されます。

[クリック] 料金の詳細については、こちらをのリンクを参照してください。 サービスとリージョンごとの正確な価格について確認してください

Azure を初めて使用する方で、これらのサービスを試したい場合は、こちらのリンクを使用してサインアップすると $200 を入手できます。

(このスライドは、cognitive services の価格の "モデル" となる一般的な概要を提供することを目的としています。 参加者は、指定されたリンクから、使用するサービスの正確な価格を確認する必要があります。)

### <a name="slide-data-considerations"></a>スライド:データに関する考慮事項

また、データの送信先とその使用方法についても考慮する必要があります。

画像やテキストなどのデータは、推論を行うために Azure にアップロードされますが、Cognitive Services に保存されることはありません。 このリンクから、プライバシーと規制遵守に関するすべての詳細情報をご確認いただけます。 しかし、ファイアウォールを超えてデータを転送することができない医療業界に従事している場合は、コンテナーという選択肢もあります。

### <a name="slide-deployment-with-containers"></a>スライド:コンテナーを使用したデプロイ

一部の Cognitive Services は、独立したコンテナーとして使用できます。 コンテナー画像をダウンロードし、ファイアウォールの内側にデプロイして、Azure と同じように提供されるローカル エンドポイントを使用するだけです。 違いは、データが独自のネットワークから外に送信されることはないということです。 コンテナーが Azure に接続する唯一の場合が、課金のためです。使用量は、Azure 自体とまったく同じ方法で課金されます。

### <a name="slide-ethical-considerations"></a>スライド:倫理的な考慮事項

最後に、最も重要なスライドを残してあります。 ここまで、強力な AI 機能をアプリケーションに簡単に統合できることを見てきました。 しかし、大いなる力には大いなる責任が伴うように、アプリケーションがユーザーに与える影響を理解し、倫理的な影響を考慮することが非常に重要です。

AI テクノロジを使用している場合は、次のような倫理的なフレームワーク内で作業する必要があります。

* 人間を AI に置き換えるのではなく、ユーザーが既に行っている内容をより多く*達成できるように*することに焦点を当てる

* すべての種類のユーザーを*含め*、すべてのユーザーがアプリケーションから恩恵を受けることができる

* 公正かつ透明性があり、特にマイノリティのグループを阻害しないこと。 前に学んだことを思い出してください。AI は、トレーニングされたデータ以上の成果を発揮することはできません。また、アプリケーションは、ユーザーの身分や見た目に関係なく、すべてのユーザーに対して動作することを確認する必要があります。

倫理的なフレームワークをお持ちでない場合は、こちらのリンクから確認できる、人工知能に関する Microsoft 独自の原則から開始することをお勧めします。

### <a name="slide-wrapping-up"></a>スライド:まとめ

事前構築された AI を使えば、人間的な機能を簡単に追加できます。 事前構築済みモデルですべてを行うことはできませんが、素早く結果を出すことができます。 このラーニング パスの後半では、残りの 20% のカスタム モデルについて説明します。

AI は強力ですが、魔法ではありません。 AI は非常に単純な数学を中心に据え、データによって駆動されます。 データを常に考慮し、何が起こっているかを理解するために使用します。 特に、最高の AI でも、トレーニング データで適切に表現されていないグループについては、間違う可能性があることに注意してください。

最後に試してみましょう。 使用を開始するために多くの専門知識は必要ありませんが、すべてのユーザーが AI の倫理的な影響とユーザーへの影響について理解している必要があります。そのため、AI を使用するための倫理的なフレームワークを開発し、それに従うようにしてください。

### <a name="slide-docs-alert"></a>スライド:/Docs のお知らせ

ファースト ステップ ガイドやリファレンスなど、Azure Cognitive Services の詳細については、Microsoft Docs を参照してください。

### <a name="slide-ms-learn-alert"></a>スライド:/MS Learn のお知らせ

Cognitive Services の使用方法について学習したい場合は、Microsoft Learn の無料コースをご活用ください。ステップバイステップで学ぶことができます。

### <a name="slide-resources"></a>スライド:リソース

Docs and Learn へのリンクと、このプレゼンテーションで説明したすべてのリソースについては、このスライドのセッション リソースのリンクをご覧ください。 また、この github リポジトリで提供されているコードとスクリプトを使用して、本日お見せしたデモをご自身で実行することもできます。 AI またはデータ サイエンスで Microsoft の認定を受けたい場合は、本日の参加者の皆様向けの無料の認定資格付与の特別プランをご利用ください。詳細については、こちらのリンクをご覧ください。

ありがとうございました。
