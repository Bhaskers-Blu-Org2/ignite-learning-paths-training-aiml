{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import *\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Workspace, Datastore, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  \n",
    "#  \n",
    "# Connect to environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Workspace:\",ws.name,\"in region\", ws.location)\n",
    "\n",
    "# Connect to compute cluster\n",
    "cluster = ComputeTarget(workspace=ws, name=\"absa-cluster\")\n",
    "print('Compute cluster:', cluster.name)\n",
    "\n",
    "# Connect to the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(\"Datastore:\",ds.name)\n",
    "\n",
    "# Connect to the experiment\n",
    "experiment = Experiment(workspace=ws, name='absa_hyperdrive')\n",
    "print(\"Experiment:\",experiment.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   \n",
    "#  \n",
    "# Fine-Tuning the model  with AzureML HyperDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters that need to be tuned\n",
    "param_sampling = RandomParameterSampling({\n",
    "         '--asp_thresh': choice(range(2,5)),\n",
    "         '--op_thresh': choice(range(2,5)), \n",
    "         '--max_iter': choice(range(2,5))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a termination policy\n",
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data_folder': ds,\n",
    "    '--large': 'yes'\n",
    "}\n",
    "\n",
    "# Create the estimator\n",
    "nlp_est = Estimator(source_directory='../scripts',\n",
    "                   compute_target=cluster,\n",
    "                   script_params=script_params,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa',\n",
    "                                 'spacy==2.1.8']\n",
    ")\n",
    "\n",
    "# Create the HyperDriveConfig\n",
    "hd_config = HyperDriveConfig(estimator=nlp_est,\n",
    "                            hyperparameter_sampling=param_sampling,\n",
    "                            #policy=early_termination_policy,\n",
    "                            primary_metric_name='f1_weighted',\n",
    "                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                            max_total_runs=16,\n",
    "                            max_concurrent_runs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the estimators by submitting the HyperDriveConfig\n",
    "hyperdrive_run = experiment.submit(hd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the HyperDrive runs\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a previous run and show the details\n",
    "hyperdrive_run = [r for r in experiment.get_runs() if r.id == 'absa_hyperdrive_1580325134380722'][0]\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "\n",
    "print('Best is Run is:',best_run.number,' \\n  F1: {0:.5f}'.format(best_run_metrics['f1_weighted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}